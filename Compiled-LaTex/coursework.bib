%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for 赵梓彤 at 2023-08-22 15:02:36 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@book{kochmar2022getting,
	author = {Ekaterina Kochmar},
	date-added = {2023-08-22 14:49:25 +0100},
	date-modified = {2023-08-22 14:53:09 +0100},
	month = {September},
	publisher = {Manning Publications},
	series = {ISBN 9781617296765},
	title = {Getting Started with Natural Language Processing},
	year = {2022}}

@article{Papadatos:2014aa,
	abstract = {The large increase in the number of scientific publications has fuelled a need for semi- and fully automated text mining approaches in order to assist in the triage process, both for individual scientists and also for larger-scale data extraction and curation into public databases. Here, we introduce a document classifier, which is able to successfully distinguish between publications that are `ChEMBL-like'(i.e. related to small molecule drug discovery and likely to contain quantitative bioactivity data) and those that are not. The unprecedented size of the medicinal chemistry literature collection, coupled with the advantage of manual curation and mapping to chemistry and biology make the ChEMBL corpus a unique resource for text mining.},
	author = {Papadatos, George and van Westen, Gerard JP and Croset, Samuel and Santos, Rita and Trubian, Simone and Overington, John P.},
	date = {2014/08/12},
	date-added = {2023-08-22 13:39:36 +0100},
	date-modified = {2023-08-22 13:39:36 +0100},
	doi = {10.1186/s13321-014-0040-8},
	id = {Papadatos2014},
	isbn = {1758-2946},
	journal = {Journal of Cheminformatics},
	number = {1},
	pages = {40},
	title = {A document classifier for medicinal chemistry publications trained on the ChEMBL corpus},
	url = {https://doi.org/10.1186/s13321-014-0040-8},
	volume = {6},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1186/s13321-014-0040-8}}

@article{Varghese:2018aa,
	abstract = {Machine learning and natural language processing algorithms are currently widely used to retrieve relevant documents in a variety of contexts, including literature review and systematic review. Supervised machine learning algorithms perform well in terms of retrieval metrics such as recall and precision, but require the use of a sizeable training dataset, which is typically expensive to develop. Unsupervised machine learning algorithms do not require a training dataset and may perform well in terms of recall, but are typically lower in precision, and do not offer a transparent means for decision-makers to justify selection choices. In this paper, we illustrate the use of a hybrid document classification method based on semi-supervised learning that we refer to as ``supervised clustering.''We show that supervised clustering combines the ease of use of unsupervised algorithms with the retrieval efficiency and transparency of supervised algorithms. We demonstrate through simulations the high performance and unbiased predictions of supervised clustering when provided even with only minimal training data. We further propose the use of ensemble learning as a means to maximize retrieval efficiency and to prioritize the review of those documents that are not eliminated by the supervised clustering algorithm.},
	author = {Varghese, Arun and Cawley, Michelle and Hong, Tao},
	date = {2018/09/01},
	date-added = {2023-08-22 13:32:02 +0100},
	date-modified = {2023-08-22 13:32:02 +0100},
	doi = {10.1007/s10669-017-9670-5},
	id = {Varghese2018},
	isbn = {2194-5411},
	journal = {Environment Systems and Decisions},
	number = {3},
	pages = {398--414},
	title = {Supervised clustering for automated document classification and prioritization: a case study using toxicological abstracts},
	url = {https://doi.org/10.1007/s10669-017-9670-5},
	volume = {38},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1007/s10669-017-9670-5}}

@article{Ombabi:2020aa,
	abstract = {Recently, the world has witnessed an exponential growth of social networks which have opened a venue for online users to express and share their opinions in different life aspects. Sentiment analysis has become a hot-trend research topic in the field of natural language processing due to its significant roles in analyzing the public's opinion and deriving effective opinion-based decisions. Arabic is one of the widely used languages across social networks. However, its morphological complexities and varieties of dialects make it a challenging language for sentiment analysis. Therefore, inspired by the success of deep learning algorithms, in this paper, we propose a novel deep learning model for Arabic language sentiment analysis based on one layer CNN architecture for local feature extraction, and two layers LSTM to maintain long-term dependencies. The feature maps learned by CNN and LSTM are passed to SVM classifier to generate the final classification. This model is supported by FastText words embedding model. Extensive experiments carried out on a multi-domain corpus demonstrate the outstanding classification performance of this model with an accuracy of 90.75{\%}. Furthermore, the proposed model is validated using different embedding models and classifiers. The results show that FastText skip-gram model and SVM classifier are more valuable alternatives for the Arabic sentiment analysis. The proposed model outperforms several well-established state-of-the-art approaches on relevant corpora with up to {\$}{\$}+{$\backslash$},20.71{$\backslash$}{\%}{\$}{\$}accuracy improvement.},
	author = {Ombabi, Abubakr H. and Ouarda, Wael and Alimi, Adel M.},
	date = {2020/07/05},
	date-added = {2023-08-22 13:17:41 +0100},
	date-modified = {2023-08-22 13:17:41 +0100},
	doi = {10.1007/s13278-020-00668-1},
	id = {Ombabi2020},
	isbn = {1869-5469},
	journal = {Social Network Analysis and Mining},
	number = {1},
	pages = {53},
	title = {Deep learning CNN--LSTM framework for Arabic sentiment analysis using textual information shared in social networks},
	url = {https://doi.org/10.1007/s13278-020-00668-1},
	volume = {10},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s13278-020-00668-1}}

@article{Mohammed:2019aa,
	abstract = {Social media are considered an excellent source of information and can provide opinions, thoughts and insights toward various important topics. Sentiment analysis becomes a hot topic in research due to its importance in making decisions based on opinions derived from analyzing the user's contents on social media. Although the Arabic language is one of the widely spoken languages used for content sharing across the social media, the sentiment analysis on Arabic contents is limited due to several challenges including the morphological structures of the language, the varieties of dialects and the lack of the appropriate corpora. Hence, the rapid increase in research in Arabic sentiment analysis is grown slowly in contrast to other languages such as English. The contribution of this paper is twofold: First, we introduce a corpus of forty thousand labeled Arabic tweets spanning several topics. Second, we present three deep learning models, namely CNN, LSTM and RCNN, for Arabic sentiment analysis. With the help of word embedding, we validate the performance of the three models on the proposed corpus. The experimental results indicate that LSTM with an average accuracy of 81.31{\%} outperforms CNN and RCNN. Also, applying data augmentation on the corpus increases LSTM accuracy by 8.3{\%}.},
	author = {Mohammed, Ammar and Kora, Rania},
	date = {2019/09/21},
	date-added = {2023-08-22 13:15:18 +0100},
	date-modified = {2023-08-22 13:15:18 +0100},
	doi = {10.1007/s13278-019-0596-4},
	id = {Mohammed2019},
	isbn = {1869-5469},
	journal = {Social Network Analysis and Mining},
	number = {1},
	pages = {52},
	title = {Deep learning approaches for Arabic sentiment analysis},
	url = {https://doi.org/10.1007/s13278-019-0596-4},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s13278-019-0596-4}}

@article{Wang:2020aa,
	abstract = {Recently, some statistic topic modeling approaches, e.g., Latent Dirichlet allocation (LDA), have been widely applied in the field of document classification. However, standard LDA is a completely unsupervised algorithm, and then there is growing interest in incorporating prior information into the topic modeling procedure. Some effective approaches have been developed to model different kinds of prior information, for example, observed labels, hidden labels, the correlation among labels, label frequencies; however, these methods often need heavy computing because of model complexity. In this paper, we propose a new supervised topic model for document classification problems, Twin Labeled LDA (TL-LDA), which has two sets of parallel topic modeling processes, one incorporates the prior label information by hierarchical Dirichlet distributions, the other models the grouping tags, which have prior knowledge about the label correlation; the two processes are independent from each other, so the TL-LDA can be trained efficiently by multi-thread parallel computing. Quantitative experimental results compared with state-of-the-art approaches demonstrate our model gets the best scores on both rank-based and binary prediction metrics in solving single-label classification, and gets the best scores on three metrics, i.e., One Error, Micro-F1, and Macro-F1 while multi-label classification, including non power-law and power-law datasets. The results show benefit from modeling fully prior knowledge, our model has outstanding performance and generalizability on document classification. Further comparisons with recent works also indicate the proposed model is competitive with state-of-the-art approaches.},
	author = {Wang, Wei and Guo, Bing and Shen, Yan and Yang, Han and Chen, Yaosen and Suo, Xinhua},
	date = {2020/12/01},
	date-added = {2023-08-22 13:00:55 +0100},
	date-modified = {2023-08-22 13:00:55 +0100},
	doi = {10.1007/s10489-020-01798-x},
	id = {Wang2020},
	isbn = {1573-7497},
	journal = {Applied Intelligence},
	number = {12},
	pages = {4602--4615},
	title = {Twin labeled LDA: a supervised topic model for document classification},
	url = {https://doi.org/10.1007/s10489-020-01798-x},
	volume = {50},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s10489-020-01798-x}}

@inproceedings{Liu2015,
	abstract = {Latent Dirichlet Allocation (LDA) probabilistic topic model is a very effective dimension-reduction tool which can automatically extract latent topics and dedicate to text representation in a lower-dimensional semantic topic space. But the original LDA and its most variants are unsupervised without reference to category label of the documents in the training corpus. And most of them view the terms in vocabulary as equally important, but the weight of each term is different, especially for a skewed corpus in which there are many more samples of some categories than others. As a result, we propose a supervised parameter estimation method based on category and document information which can estimate the parameters of LDA according to term weight. The comparative experiments show that the proposed method is superior for the skewed text classification, which can largely improve the recall and precision of the minority category.},
	address = {Cham},
	author = {Zhenyan, Liu and Dan, Meng and Weiping, Wang and Chunxia, Zhang},
	booktitle = {Web Technologies and Applications},
	date-added = {2023-08-22 12:01:10 +0100},
	date-modified = {2023-08-22 12:01:29 +0100},
	editor = {Cheng, Reynold and Cui, Bin and Zhang, Zhenjie and Cai, Ruichu and Xu, Jia},
	isbn = {978-3-319-25255-1},
	pages = {401--410},
	publisher = {Springer International Publishing},
	title = {A Supervised Parameter Estimation Method of LDA},
	year = {2015}}

@inproceedings{Kang2014,
	abstract = {We propose Hetero-Labeled LDA (hLLDA), a novel semi-supervised topic model, which can learn from multiple types of labels such as document labels and feature labels (i.e., heterogeneous labels), and also accommodate labels for only a subset of classes (i.e., partial labels). This addresses two major limitations in existing semi-supervised learning methods: they can incorporate only one type of domain knowledge (e.g. document labels or feature labels), and they assume that provided labels cover all the classes in the problem space. This limits their applicability in real-life situations where domain knowledge for labeling comes in different forms from different groups of domain experts and some classes may not have labels. hLLDA resolves both the label heterogeneity and label partialness problems in a unified generative process.},
	address = {Berlin, Heidelberg},
	author = {Kang, Dongyeop and Park, Youngja and Chari, Suresh N.},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	date-added = {2023-08-22 11:50:52 +0100},
	date-modified = {2023-08-22 11:51:15 +0100},
	editor = {Calders, Toon and Esposito, Floriana and H{\"u}llermeier, Eyke and Meo, Rosa},
	isbn = {978-3-662-44848-9},
	pages = {640--655},
	publisher = {Springer Berlin Heidelberg},
	title = {Hetero-Labeled LDA: A Partially Supervised Topic Model with Heterogeneous Labels},
	year = {2014}}

@inproceedings{Huang2017,
	author = {Huang, Ling and Ma, Jinyu and Chen, Chunling},
	booktitle = {2017 24th Asia-Pacific Software Engineering Conference Workshops (APSECW)},
	date-added = {2023-08-21 23:03:58 +0100},
	date-modified = {2023-08-21 23:05:02 +0100},
	pages = {71-77},
	title = {Topic Detection from Microblogs Using T-LDA and Perplexity},
	year = {2017}}

@misc{vanAtteveldt2018,
	author = {Wouter van Atteveldt},
	date-added = {2023-08-21 22:25:35 +0100},
	date-modified = {2023-08-21 22:26:44 +0100},
	howpublished = {\url{https://i.amcat.nl/lda/2_lda.html}},
	month = {February},
	title = {Fitting LDA Models in R},
	year = {2018}}

@inproceedings{Lakshminarayanan2011,
	author = {Lakshminarayanan, Balaji and Raich, Raviv},
	booktitle = {2011 IEEE International Workshop on Machine Learning for Signal Processing},
	date-added = {2023-08-21 21:59:49 +0100},
	date-modified = {2023-08-21 22:01:39 +0100},
	pages = {1-6},
	title = {Inference in Supervised latent Dirichlet allocation},
	year = {2011}}

@misc{Kapadia2019,
	author = {Shashank Kapadia},
	date-added = {2023-08-21 21:03:22 +0100},
	date-modified = {2023-08-21 21:49:12 +0100},
	howpublished = {\url{https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0}},
	month = {August},
	title = {Evaluate Topic Models: Latent Dirichlet Allocation (LDA)},
	year = {2019}}

@misc{Kulshrestha2019,
	author = {Ria Kulshrestha},
	date-added = {2023-08-21 20:30:15 +0100},
	date-modified = {2023-08-21 21:48:14 +0100},
	howpublished = {\url{https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2}},
	month = {July},
	title = {A Beginner's Guide to Latent Dirichlet Allocation(LDA)},
	year = {2019}}

@misc{Naushan2020,
	author = {Haaya Naushan},
	date-added = {2023-08-21 20:19:54 +0100},
	date-modified = {2023-08-21 21:52:49 +0100},
	howpublished = {\url{https://towardsdatascience.com/topic-modeling-with-latent-dirichlet-allocation-e7ff75290f8}},
	month = {December},
	title = {Topic Modeling with Latent Dirichlet Allocation},
	year = {2020}}

@article{Jelodar:2019aa,
	abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data and text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modelling; Latent Dirichlet Allocation (LDA) is one of the most popular in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper will be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated highly scholarly articles (between 2003 to 2016) related to topic modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. In addition, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
	author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
	date = {2019/06/01},
	date-added = {2023-08-21 16:11:19 +0100},
	date-modified = {2023-08-21 16:11:19 +0100},
	doi = {10.1007/s11042-018-6894-4},
	id = {Jelodar2019},
	isbn = {1573-7721},
	journal = {Multimedia Tools and Applications},
	number = {11},
	pages = {15169--15211},
	title = {Latent Dirichlet allocation (LDA) and topic modeling: models, applications, a survey},
	url = {https://doi.org/10.1007/s11042-018-6894-4},
	volume = {78},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1007/s11042-018-6894-4}}

@misc{Seth2021,
	author = {Neha Seth},
	date-added = {2023-08-18 00:57:03 +0100},
	date-modified = {2023-08-21 21:52:11 +0100},
	howpublished = {\url{https://www.analyticsvidhya.com/blog/2021/06/part-2-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/}},
	month = {June},
	title = {Topic Modeling and Latent Dirichlet Allocation (LDA) using Gensim and Sklearn},
	year = {2021}}

@misc{Selivanov2016,
	annote = {R Package Version},
	author = {Dmitriy Selivanov},
	date-added = {2023-08-11 23:28:44 +0100},
	date-modified = {2023-08-11 23:40:27 +0100},
	howpublished = {https://search.r-project.org/CRAN/refmans/text2vec/html/coherence.html},
	read = {1},
	title = {text2vec: Coherence metrics for topic models},
	year = {2016}}

@inproceedings{ramage2009labeled,
	author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D},
	booktitle = {Proceedings of the 2009 conference on empirical methods in natural language processing},
	date-added = {2023-08-02 16:37:50 +0100},
	date-modified = {2023-08-02 16:37:50 +0100},
	pages = {248--256},
	title = {Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora},
	year = {2009}}

@inproceedings{wallach2009evaluation,
	author = {Wallach, Hanna M and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	date-added = {2023-08-02 16:37:46 +0100},
	date-modified = {2023-08-02 16:37:46 +0100},
	pages = {1105--1112},
	title = {Evaluation methods for topic models},
	year = {2009}}

@article{blei2012probabilistic,
	author = {Blei, David M},
	date-added = {2023-08-02 16:37:43 +0100},
	date-modified = {2023-08-02 16:37:43 +0100},
	journal = {Communications of the ACM},
	number = {4},
	pages = {77--84},
	publisher = {ACM New York, NY, USA},
	title = {Probabilistic topic models},
	volume = {55},
	year = {2012}}

@article{blei2003latent,
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	date-added = {2023-08-02 16:35:41 +0100},
	date-modified = {2023-08-02 16:36:45 +0100},
	journal = {Journal of machine Learning research},
	number = {Jan},
	pages = {993--1022},
	title = {Latent dirichlet allocation},
	volume = {3},
	year = {2003}}
