{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zhaozitong/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhaozitong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/zhaozitong/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "['titles-abstracts.csv UKRI']\n",
      "titles-abstracts.csv\n",
      "\n",
      "chunk 0\n",
      "\n",
      "chunk 1\n",
      "\n",
      "chunk 2\n",
      "\n",
      "chunk 3\n",
      "\n",
      "chunk 4\n",
      "\n",
      "chunk 5\n",
      "\n",
      "chunk 6\n",
      "\n",
      "chunk 7\n",
      "\n",
      "chunk 8\n",
      "\n",
      "chunk 9\n",
      "\n",
      "chunk 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !/bin/bash\n",
    "\n",
    "# process text  \n",
    "#echo 'tokenising text'\n",
    "# first argument is the language to use; second arg is the path to the python script; third arg is the path to where get the data from; \n",
    "!python3 tokenize_texts4mallet.py titles-abstracts-dir.txt clean-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !/bin/bash\n",
    "\n",
    "!echo filtering data\n",
    "# call helper script to filter minimum number of tokens\n",
    "# first argument below is the language of the script being called; second argument is the path to where the script is located; thrid argument is where to get the\n",
    "# input data from; fourth argument is where to save the filtered data to. \n",
    "!python3 filter-tokens.py clean-data/titles-abstracts-tokenized.csv clean-data/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('clean-data/titles-abstracts-tokenized-filtered.csv', header=None, names=['Column'])\n",
    "df['ProjectId'] = df['Column'].apply(lambda x: x[:x.find('UKRI')].strip())\n",
    "df['words'] = df['Column'].apply(lambda x: x[x.find('UKRI')+4:].strip())\n",
    "\n",
    "df.drop('Column', axis=1, inplace=True)    \n",
    "df.to_csv('temp_titles-abstracts-tokenized-filtered.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('clean-data/UKRI-project-metadata.csv',usecols=['ProjectId','StartDate','EndDate','FundingAmount','FundingBody'])\n",
    "df['ProjectId']= df['ProjectId'].str.replace(\" \", \"-\", regex=True)\n",
    "df['FundingBody']= df['FundingBody'].str.replace(\" \", \"-\", regex=True)\n",
    "df = df[df['FundingAmount'] > 0].reset_index(drop=True)\n",
    "\n",
    "df['ProjectId']= df1['ProjectId'].str.replace(\" \", \"-\", regex=True)\n",
    "df['Duration'] = ''\n",
    "df['Fundingpy'] = ''\n",
    "\n",
    "df['StartDate'] = pd.to_datetime(df['StartDate'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "df['EndDate'] = pd.to_datetime(df['EndDate'], format=\"%d/%m/%Y\", errors='coerce')\n",
    "\n",
    "\n",
    "df['Duration'] = df['EndDate'] - df['StartDate']\n",
    "df = df.drop(['StartDate', 'EndDate'], axis=1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df['FundingAmount'] = df['FundingAmount'].astype(int)\n",
    "df['Duration'] = df['Duration'].astype(str).str.replace(' days', '')\n",
    "df['Duration'] = df['Duration'].astype(int)\n",
    "df['Duration'] = pd.to_numeric(df['Duration'], errors='coerce')\n",
    "df['Fundingpy'] = df['FundingAmount'] / (df['Duration']/365)\n",
    "\n",
    "df['Fundingpy'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(subset=['Fundingpy'], inplace=True)\n",
    "df['Fundingpy']=df['Fundingpy'].astype(int)\n",
    "df = df[df['Duration'] > 0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "df.to_csv('duration-funding.csv', index = False)\n",
    "\n",
    "df1 =pd.read_csv(\"temp_titles-abstracts-tokenized-filtered.csv\")\n",
    "\n",
    "merged_df = pd.merge(df1, df, on='ProjectId', how='left')\n",
    "\n",
    "merged_df = merged_df[['ProjectId', 'Duration', 'FundingBody','FundingAmount','Fundingpy','words']]\n",
    "\n",
    "merged_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "merged_df[['ProjectId','FundingBody','words']].to_csv('funder-titles-abstracts-tokenized-filtered.csv',index=False)\n",
    "merged_df[['ProjectId','Duration','words']].to_csv('duration-titles-abstracts-tokenized-filtered.csv',index=False)\n",
    "merged_df[['ProjectId','Fundingpy','words']].to_csv('funding-titles-abstracts-tokenized-filtered.csv',index=False)\n",
    "\n",
    "merged_df[['ProjectId','Fundingpy','FundingBody','Duration','words','FundingAmount']].to_csv('label-titles-abstracts-tokenized-filtered.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
